1. Capture 

영상을 실시간으로 받으면서 원시 데이터(raw data)를 생성합니다.



2. Encoding

CPU/GPU 전용 하드웨어 인코더로 압축을 합니다. 뷰릭스의 경우 CPU 전용 드라이버로 H264방식을 사용하여 영상을 인코딩합니다. 인코딩과 동시에 영상을 특정 초 단위로 패키징(압축)하여 전송합니다.

영상 정보는 수많은 이미지의 집합이므로 이것을 그대로 전송하는 건 비효율적입니다.

이렇게 인코딩하는 기술 중 코덱이 있고, 코덱의 한 종류로 H.264방식을 뷰릭스에서 사용합니다.

코덱은 I-Frame, P-Frame, B-Frame을 활용하여 동영상을 압축합니다.

I-Frame이 완전한 이미지이고, P는 직전 프레임들의 변화량, B는 앞, 뒤 프레임들의 변화량을 참고합니다.

즉, 그 어느 영상이던지 간에, 프레임을 I, P, B 프레임으로 나누어 활용합니다.

VFS도 마찬가지로, 이러한 정보들을 담고 있고, VFS의 헤더더는 이러한 인코딩에 필요한 기초적인 프레임의 메타데이터를 제공합니다.



3. Transport

뷰릭스에선 영상 전송 프로토콜로 RTSP를 사용합니다. RTSP에서는 실제로 직접 전송하는 것이 아니라 클라이언트-서버 간의 세션을 제어하는 프로토콜로, 실제 영상 전송은 RTP(Real-Time Protocol)가 합니다. 이것이 inno_rtsp_driver가 하는 역할입니다.

앞서 말했듯이, 프레임들은 코덱을 통해 압축 과정을 거치게 됩니다. 그리고 트랙이라는 데이터 단위로 분리되어 있고, 이를 다시 묶어서 재생에 필요한 메타데이터를 추가하여 하나의 컨테이너로 묶습니다. 즉, 컨테이너는 하나의 파일이며, 해당 파일의 정보를 헤더에 저장하고, 각 프레임들을 트랙이라는 단위로 나뉩니다.

VFS도 마찬가지로, VFS헤더에 메타데이터가 존재하고, (프레임 헤더 + 프레임 데이터)라는 단위로 묶이는데, 이것이 트랙이라고 보면 됩니다.

영상 전송 시에, base64를 사용하여 컨테이너를 변환합니다. 일종의 “포장”을 하여 안전하게 전송될 수 있도록 합니다.



4. 중계 서버

미디어 서버에서 영상을 받아서 복제하여 여러 중계 서버의 클라이언트에게 전송을 합니다.



5. 수신

클라이언트가 Stream(영상)을 수신합니다. 이 때, 프로토콜이 일치해야 합니다. 뷰릭스 시스템에서는 RTSP를 사용하므로 RTSP에 맞춰야 합니다. Base64로 형 변환을 풀어 컨테이너 패킷들을 얻습니다.

컨테이너 패킷들 중에서 컨테이너 헤더를 찾아서 영상 형식에 맞게 재조립합니다.



6. Playback(재생)

디코더를 통해 압축을 풀고 화면과 소리를 출력합니다. 이 때, 영상 프레임을 받는 버퍼(뷰릭스에서는 큐를 사용하여 구현)를 최대한 작게 유지하여 지연을 최소화 시킵니다.
